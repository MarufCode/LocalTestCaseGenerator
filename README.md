# ğŸš€ Local Test Case Generator (Ollama)

A privacy-focused, local AI tool that generates comprehensive test cases from user requirements using an offline LLM (**Ollama**).

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    User[ğŸ‘¤ User] -->|1. Enters Requirements| UI[ğŸ’» React Frontend\n(Localhost:3000)]
    UI -->|2. POST /api/generate| Server[ğŸ›¡ï¸ Node.js Backend\n(Express)]
    
    subgraph Local Machine
        Server -->|3. prompt_template| Templates[ğŸ“ Prompt Engine]
        Templates -->|4. Formatted Prompt| Server
        Server -->|5. HTTP Request (Stream)| Ollama[ğŸ¦™ Ollama API\n(127.0.0.1:11434)]
        Ollama -->|6. Streaming Tokens| Server
        Server -->|7. Streaming Response| UI
    end
    
    Ollama -.-> Model[ğŸ§  Model: qwen3:4b]
```

## âœ¨ Features
- **100% Local**: No data leaves your machine. Uses local Ollama instance.
- **Privacy First**: Securely generate test cases for sensitive projects.
- **Streaming UI**: Real-time text generation response (no timeouts).
- **Premium Design**: Glassmorphism UI with dark mode.
- **Architecture**: A.N.T. 3-Layer Architecture (Architecture, Navigation, Tools).

## ğŸ› ï¸ Prerequisites

1.  **Node.js** (v18+)
2.  **Ollama**: [Download Here](https://ollama.com/)
3.  **Model**: Pull the model used (default `qwen3:4b`):
    ```bash
    ollama pull qwen3:4b
    ```

## ğŸš€ Getting Started

### 1. Clone the Repository
```bash
git clone https://github.com/MarufCode/LocalTestCaseGenerator.git
cd LocalTestCaseGenerator
```

### 2. Install Dependencies
You need to install dependencies for both the Backend and Frontend.

**Backend:**
```bash
cd server
npm install
```

**Frontend:**
```bash
cd ../client
npm install
npm run build
```

### 3. Run the App
The backend serves the frontend, so you only need to start the server.

```bash
# From the /server directory
npm start
```

Open your browser to: **[http://localhost:3000](http://localhost:3000)**

## ğŸ”§ Configuration
- **Model**: Change the model in `client/src/App.jsx` or `server/server.js` (default: `qwen3:4b`).
- **Prompt**: Edit `server/prompt_template.js` to change how test cases are generated.

## ğŸ¤ Troubleshooting
- **Result Stalls?** Restart Ollama app.
- **Connection Refused?** Ensure Ollama is running on port 11434.

---
*Generated by Antigravity*
